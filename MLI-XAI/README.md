# Machine Learning Explainability (MLI or XAI = No more blackbox) #

## LIME ##

<table class="table table-striped table-bordered table-vcenter">
    <tr>
        <td align="center"><b>üî•&nbsp;YouTube Video:&nbsp; Just apply model explanation with LIME to explain, trust and validate your model predictions</b></td>
    </tr>
    <tr>
        <td>
            <div>
                
[![Just apply model explanation with LIME to explain, trust and validate your model predictions](https://img.youtube.com/vi/Op2M5CpJehM/0.jpg)](https://www.youtube.com/watch?v=Op2M5CpJehM)

  </tr>
</table>
  
<div align="center">
  <hr/>
  <img src="https://www.oreilly.com/content/wp-content/uploads/sites/2/2019/06/Figure-6-c8db425eefec7cff5a3cf035a40d8841.jpg" width="800" />
  <h6>Figure 6. Explanation for a prediction from Inception. The top three predicted classes are ‚Äútree frog,‚Äù ‚Äúpool table,‚Äù and ‚Äúballoon.‚Äù Sources: Marco Tulio Ribeiro, Pixabay (frog, billiards, hot air balloon).</h6>
  <hr/>
  <h6>Image Source: https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/</h6>
</div> 

Quick Links:
- https://ema.drwhy.ai/LIME.html
- https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/
- https://dl.acm.org/doi/10.1145/3387166
- https://towardsdatascience.com/deep-learning-model-interpretation-using-shap-a21786e91d16
- https://towardsdatascience.com/explainable-ai-understanding-the-decisions-of-a-convolutional-neural-network-part-1-1a9cf26364fd
- https://medium.com/ing-blog/model-explainability-how-to-choose-the-right-tool-6c5eabd1a46a

GitHub Resources:
- https://github.com/marcotcr/lime
- https://github.com/slundberg/shap
- https://github.com/RikKraanVantage/explainable_AI
